{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LSTM-CRF tutorial for POS task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/nndl/anaconda3/envs/hazelnutsgz/lib/python3.6/site-packages/tqdm-4.28.1-py3.6.egg/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.modules.encoder.lstm import LSTM\n",
    "from fastNLP.modules.encoder.linear import Linear\n",
    "from fastNLP.modules.encoder.embedding import Embedding\n",
    "from fastNLP.modules.decoder.CRF import ConditionalRandomField\n",
    "from fastNLP.io.dataset_loader import Conll2003Loader\n",
    "from fastNLP.models.base_model import BaseModel\n",
    "from fastNLP import Vocabulary\n",
    "from fastNLP.modules import decoder, encoder\n",
    "from fastNLP.modules.utils import seq_mask\n",
    "from fastNLP.core.metrics import MetricBase\n",
    "from fastNLP import Trainer, Tester\n",
    "from fastNLP import AccuracyMetric\n",
    "from fastNLP.core.optimizer import SGD\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BiLSTM CRF model\n",
    "The model consists of two main layers, the first is the LSTM layers, which provide a softmax ouput for CRF layer to calculate the score loss and infer the right route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMCRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(BiLSTMCRF, self).__init__()\n",
    "        vocab_size = config[\"vocab_size\"]\n",
    "        word_emb_dim = config[\"word_emb_dim\"]\n",
    "        hidden_dim = config[\"rnn_hidden_units\"]\n",
    "        num_classes = config[\"num_classes\"]\n",
    "        bi_direciton = config[\"bi_direction\"]\n",
    "        self.Embedding = Embedding(vocab_size, word_emb_dim)\n",
    "        self.Lstm = LSTM(word_emb_dim, hidden_dim, bidirectional=bi_direciton)\n",
    "        self.Linear = Linear(2*hidden_dim if bi_direciton else hidden_dim, num_classes)\n",
    "        self.Crf = ConditionalRandomField(num_classes)\n",
    "        self.mask = None\n",
    "        \n",
    "\n",
    "    def forward(self, token_index_list, origin_len, speech_index_list=None):\n",
    "        \"\"\"\n",
    "        \n",
    "            param: token_index_list [batch_size, padded_len]: The word2index list\n",
    "            param: origin_len [batch_size]: The origin length of the sentence in each batch\n",
    "            param: speech_index_list [batch_size, padded_len]: The expected speech tagging list for each sentences\n",
    "            ret: json-like result will be utilized by Trainer or Tester, {\"loss\": loss, \"pred\": tag_seq}\n",
    "        \n",
    "        \"\"\"\n",
    "        max_len = len(token_index_list[0])\n",
    "        self.mask = self.make_mask(token_index_list, origin_len)\n",
    "        \n",
    "        x = self.Embedding(token_index_list) # [batch_size, max_len, word_emb_dim]\n",
    "        x = self.Lstm(x) # [batch_size, max_len, hidden_size]\n",
    "        x = self.Linear(x) # [batch_size, max_len, num_classes]\n",
    "        \n",
    "        loss = None\n",
    "        ## Calculate the loss value if in the training mode(the speech_index_list is given)\n",
    "        if speech_index_list is not None:\n",
    "            total_loss = self.Crf(x, speech_index_list, self.mask) ## [batch_size, 1]\n",
    "            loss = torch.mean(total_loss)\n",
    "            \n",
    "        \n",
    "        ## Get the POS sequence(padding the sequence to equal length) \n",
    "        tag_seq = self.Crf.viterbi_decode(x, self.mask)\n",
    "        for index in range(len(tag_seq)):\n",
    "            bias = max_len - origin_len[index]\n",
    "            for i in range(origin_len[index], max_len):\n",
    "                tag_seq[index][i] = 0\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"pred\": tag_seq\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def make_mask(self, x, seq_len):\n",
    "        ## make the mask for batch-load datasets \n",
    "        batch_size, max_len = x.size(0), x.size(1)\n",
    "        mask = seq_mask(seq_len, max_len)\n",
    "        mask = mask.view(batch_size, max_len)\n",
    "        mask = mask.to(x).float()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The util function to load the data & format to Dataset that fastNLP will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    ## load the data from the textfile\n",
    "    datasets = load_data(Conll2003Loader(), [\\\n",
    "                    \"./data/conll2003/train.txt\",\n",
    "                    \"./data/conll2003/valid.txt\",\n",
    "                    \"./data/conll2003/test.txt\"\n",
    "                  ])\n",
    "    train_data = datasets[0]\n",
    "    valid_data = datasets[1]\n",
    "    test_data = datasets[2]\n",
    "    \n",
    "    #Lower case the words in the sentences\n",
    "    lower_case([train_data, valid_data, test_data], \"token_list\")\n",
    "    \n",
    "    ## Build vocab\n",
    "    vocab = build_vocab([train_data, valid_data, test_data], \"token_list\")\n",
    "    speech_vocab = build_vocab([train_data, valid_data, test_data], \"label0_list\")\n",
    "    \n",
    "    ## Build index\n",
    "    build_index([train_data, valid_data, test_data], \"token_list\", 'token_index_list', vocab)\n",
    "    build_index([train_data, valid_data, test_data], \"label0_list\", 'speech_index_list', speech_vocab)\n",
    "    \n",
    "    \n",
    "    ## Build origin length for each sentence, for mask in the following procedure\n",
    "    build_origin_len([train_data, valid_data, test_data], \"token_list\", 'origin_len')\n",
    "    \n",
    "    return train_data, valid_data, test_data, vocab, speech_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data, vocab, speech_vocab = prepare_data()\n",
    "\n",
    "## Set the corresponding tags for each dataset, which will be used in the Trainer\n",
    "train_data.set_input(\"token_index_list\", \"origin_len\", \"speech_index_list\")\n",
    "test_data.set_input(\"token_index_list\", \"origin_len\", \"speech_index_list\")\n",
    "valid_data.set_input(\"token_index_list\", \"origin_len\", \"speech_index_list\")\n",
    "\n",
    "train_data.set_target(\"speech_index_list\")\n",
    "test_data.set_target(\"speech_index_list\")\n",
    "valid_data.set_target(\"speech_index_list\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## Test the model\n",
    "tester = Tester(data=test_data, \n",
    "          model=model, \n",
    "          metrics=PosMetric(pred='pred', target='speech_index_list'),\n",
    "   )\n",
    "tester.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"vocab_size\": len(vocab),\n",
    "    \"word_emb_dim\": 200, \n",
    "    \"rnn_hidden_units\": 600,\n",
    "    \"num_classes\": len(speech_vocab),\n",
    "    \"bi_direction\": True\n",
    "}\n",
    "model = BiLSTMCRF(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce the Metric for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosMetric(MetricBase):\n",
    "    \"\"\"\n",
    "        The PosMetric use the accuracy of each word to \n",
    "        evaluate the performance of POS task, suggested by the \n",
    "        original paper on https://arxiv.org/abs/1508.01991\n",
    "    \"\"\"\n",
    "    def __init__(self, pred=None, target=None):\n",
    "        super().__init__()\n",
    "        self._init_param_map(pred=pred, target=target)\n",
    "        self.total = 0\n",
    "        self.acc_count = 0\n",
    "        \n",
    "\n",
    "    def evaluate1(self, pred, target):\n",
    "        \"\"\"\n",
    "            Each time when loading a batch of data in the Trainer&Tester, \n",
    "            this function would be called for one time. So we can use some \n",
    "            class member to memorize the state in the training process.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.acc_count += torch.sum(torch.eq(pred, target).float()).item()\n",
    "        self.total += np.prod(list(pred.size()))\n",
    "\n",
    "    def evaluate(self, pred, target):  \n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            for j in range(len(pred[0])):\n",
    "                if target[i][j] != 0:\n",
    "                    self.acc_count += 1 if target[i][j] == pred[i][j] else 0\n",
    "                    self.total += 1\n",
    "    \n",
    "    def get_metric(self):\n",
    "        \"\"\"\n",
    "            As suggested in the tutorial, this function would be called once \n",
    "            the Trainer finished 1 epoch of training on the whole dataset.\n",
    "            \n",
    "            :return {\"acc\": float}\n",
    "        \"\"\"\n",
    "        \n",
    "        return {\n",
    "            'acc': round(self.acc_count / self.total, 6)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.01) \n",
    "\n",
    "\n",
    "## Train the model\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    train_data=train_data, \n",
    "    dev_data=valid_data,\n",
    "    use_cuda=True,\n",
    "    metrics=PosMetric(pred='pred', target='speech_index_list'),\n",
    "    optimizer=optimizer,\n",
    "    n_epochs=5, \n",
    "    batch_size=100,\n",
    "    save_path=\"./save\"\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Tester(data=test_data, \n",
    "              model=model, \n",
    "              metrics=PosMetric(pred='pred', target='speech_index_list')\n",
    "       )\n",
    "test.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hazelnutsgz",
   "language": "python",
   "name": "hazelnutsgz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
